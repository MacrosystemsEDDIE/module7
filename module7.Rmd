---
title: "Macrosystems EDDIE Module 7: Using Data to Improve Ecological Forecasts"
author: "Mary Lofton, Tadhg Moore, Cayelan Carey, Quinn Thomas"
date: "2/16/2022"
output: html_document
---
## Set-up
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Load libraries ----
library(ggplot2)
library(plotly)
library(tidyverse)
library(lubridate)
```
## Define functions
### Function to read in NEON data
```{r}
# Read in NEON data ---- 
#' @param siteID character; four-letter identifier for the NEON site
#' @param var character; short name used to identify NEON variables. See "data/neon_variables.csv"

read_neon_data <- function(siteID, var) {
  idx <- which(neon_vars$Short_name == var)
  read_var <- neon_vars$id[idx]
  units <- neon_vars$units[idx]
  file <- file.path("data", "neon", paste0(siteID, "_", read_var, "_", units, ".csv"))
  if(file.exists(file)) {
    df <- read.csv(file)
    df[, 1] <- as.POSIXct(df[, 1], tz =  "UTC")
    colnames(df)[2] <- "value"
    df$var <- neon_vars$id[idx]
    return(df)
  } else {
    stop("File: '", file, "' does not exist.")
  }
}
```
### Function to create NP model inputs
```{r}
# Create np inputs (Tadhg's version) ----
# create_np_inputs <- function(time, PAR = NULL, swr = NULL, temp = NULL) {
# 
#   # year <- lubridate::year(time[1])
#   ndays <- round(as.numeric(difftime(time, time[1], units = "days")))
# 
#   # PAR calculations ----
#   if(is.null(PAR) & !is.null(swr)) {
#     PAR <- LakeMetabolizer::sw.to.par.base(swr)
#   }
# 
#   if(is.null(PAR) & is.null(swr)) {
#     PAR <- 0.5 * (540 + 440 * sin(2 * pi * ndays/365-1.4))
#   }
# 
#   if(length(temp) == 2) {
#     TEMP <- (temp[1] + temp[2] * sin(2*pi*ndays/365-1.4))
#   } else {
#     TEMP <- temp
#   }
# 
# 
#   out = data.frame(Day = ndays, PAR = PAR, TEMP = TEMP)
# 
# 
#   #Read to a .csv file
#   # write.csv(out, file.path("data", out_file), row.names = FALSE)
#   return(out)
# 
# }

# Create np inputs (MEL's version) ----
create_np_inputs <- function(drivers_df,driver_colnames) {
  #we are just doing linear interpolation for now
  
  #first loop through driver vars
  for (j in 1:length(driver_colnames)){
  NAindex <- which(is.na(drivers_df[,driver_colnames[j]]))
  
  if(length(NAindex)==0){next}
  
  NonNAindex <- which(!is.na(drivers_df[,driver_colnames[j]]))
  numNAchunks <- c(1:length(diff(NonNAindex)[diff(NonNAindex)>1]))
  
  #then loop through number of NA chunks
  for (i in numNAchunks){
    
    if(numNAchunks[i] == 1){
      firstNA <- min(NAindex)
      nextNonNA <- min(NonNAindex[NonNAindex>firstNA])
    } else {
      firstNA <- min(NAindex[NAindex>lastNonNA])
      nextNonNA <- min(NonNAindex[NonNAindex>firstNA])
    }
  
  drivers_df[c(firstNA:nextNonNA-1),driver_colnames[j]] <- approx(x = drivers_df[c(firstNA-1,nextNonNA),1],y = drivers_df[c(firstNA-1,nextNonNA),driver_colnames[j]],xout = drivers_df[c(firstNA:nextNonNA-1),1])[[2]]
  
        lastNonNA <- nextNonNA

  }
  }

  out = drivers_df


  #Read to a .csv file
  # write.csv(out, file.path("data", out_file), row.names = FALSE)
  return(out)

}

# END
```
### Function to define NP model
```{r}

# Define NP model
# this has been adapted to play nicely with Jake's EnKF functions

NP_model <- function(PHYTO, DIN, TEMP, PAR, maxUptake){

  #STATES
  curr_PHYTO = PHYTO
  curr_DIN = DIN
  
  #PARAMETERS
  maxUptake = maxUptake #day-1; this is the only parameter we are tuning
  kspar=120 #uEinst m-2 s-1
  ksdin=0.5 #mmol m-3
  maxGrazing=1.0 # day-1
  ksphyto=1 #mmol N m-3
  pFaeces=0.3 #unitless
  mortalityRate=0.4 #(mmmolN m-3)-1 day-1
  excretionRate=0.1 #day-1
  mineralizationRate=0.1 #day-1
  Chl_Nratio = 1 #mg chl (mmolN)-1
  Q10 = 2  #unitless
  refTEMP = 20

  #DRIVERS
  PAR <- PAR
  TEMP <- TEMP 

  #FLUX EQUATIONS HERE
  Temp_effect = Q10^((TEMP-refTEMP)/10)
  N_Uptake <- maxUptake*PHYTO*(PAR/(PAR+kspar))*(DIN/(DIN+ksdin))*Temp_effect

  Mortality <- mortalityRate*PHYTO^2
  Mineralization <- mineralizationRate *
    Temp_effect

  #Convert from plankton biomass to Chlorophyll to compare to data
  Chlorophyll <- PHYTO^Chl_Nratio

  PHYTO_pred = curr_PHYTO + N_Uptake - Mortality
  DIN_pred = curr_DIN + Mortality - N_Uptake #+ NLOAD + Excretion

  return(list(PHYTO_pred = PHYTO_pred,
              DIN_pred = DIN_pred,
              maxUptake = maxUptake))   # the ordinary output variables
}


```
### ENKF functions from Jake Zwart GLEON workshop
#### Function to create vector of model timesteps
```{r}
#' retreive the model time steps based on start and stop dates and time step
#'
#' @param model_start model start date in date class
#' @param model_stop model stop date in date class
#' @param time_step model time step, defaults to daily timestep
get_model_dates = function(model_start, model_stop, time_step = 'days'){
  
  model_dates = seq.Date(from = as.Date(model_start), to = as.Date(model_stop), by = time_step)
  
  return(model_dates)
}
```
#### Function to create vector to hold states and parameters for updating
```{r}
#' vector for holding states and parameters for updating
#'
#' @param n_states number of states we're updating in data assimilation routine
#' @param n_params_est number of parameters we're calibrating
#' @param n_step number of model timesteps
#' @param n_en number of ensembles
get_Y_vector = function(n_states, n_params_est, n_step, n_en){
  
  Y = array(dim = c(n_states + n_params_est, n_step, n_en))
  
  return(Y)
}
```
#### Function to create observation error matrix
```{r}
#' observation error matrix, should be a square matrix where
#'   col & row = the number of states and params for which you have observations
#'
#' @param n_states number of states we're updating in data assimilation routine
#' @param n_param_obs number of parameters for which we have observations
#' @param n_step number of model timesteps
#' @param state_sd vector of state observation standard deviation; assuming sd is constant through time
#' @param param_sd vector of parameter observation standard deviation; assuming sd is constant through time
get_obs_error_matrix = function(n_states, n_params_obs, n_step, state_sd, param_sd){
  
  R = array(0, dim = c(n_states + n_params_obs, n_states + n_params_obs, n_step))
  
  state_var = state_sd^2 #variance of temperature observations
  
  param_var = param_sd^2
  
  if(n_params_obs > 0){
    all_var = c(state_var, param_var)
  }else{
    all_var = state_var
  }
  
  for(i in 1:n_step){
    # variance is the same for each depth and time step; could make dynamic or varying by time step if we have good reason to do so
    R[,,i] = diag(all_var, n_states + n_params_obs, n_states + n_params_obs)
  }
  
  return(R)
}

```
#### Function to create matrix that identifies when observations are available
```{r}
#' Measurement operator matrix saying 1 if there is observation data available, 0 otherwise
#'
#' @param n_states number of states we're updating in data assimilation routine
#' @param n_param_obs number of parameters for which we have observations
#' @param n_params_est number of parameters we're calibrating
#' @param n_step number of model timesteps
#' @param obs observation matrix created with get_obs_matrix function
get_obs_id_matrix = function(n_states, n_params_obs, n_params_est, n_step, obs){
  
  H = array(0, dim=c(n_states + n_params_obs, n_states + n_params_est, n_step))
  
  # order goes 1) states, 2)params for which we have obs, 3) params for which we're estimating but don't have obs
  
  for(t in 1:n_step){
    H[1:(n_states + n_params_obs), 1:(n_states + n_params_obs), t] = diag(ifelse(is.na(obs[,,t]),0, 1), n_states + n_params_obs, n_states + n_params_obs)
  }
  
  return(H)
}

```
#### Function to turn observation dataframe into matrix
```{r}
#' turn observation dataframe into matrix
#'
#' @param obs_df observation data frame
#' @param model_dates dates over which you're modeling
#' @param n_step number of model time steps
#' @param n_states number of states we're updating in data assimilation routine
#' @param states character string vector of state names in obs_file

get_obs_matrix = function(obs_df, model_dates, n_step, n_states, states){
  
  # need to know location and time of observation
  
  obs_df_filtered = obs_df %>%
    dplyr::filter(as.Date(datetime) %in% model_dates) %>%
    mutate(date = as.Date(datetime)) %>%
    select(date, chla, din) %>%
    mutate(date_step = which(model_dates %in% date))

  obs_matrix = array(NA, dim = c(n_states, 1, n_step))

  for(i in 1:n_states){
  for(j in obs_df_filtered$date_step){
    obs_matrix[i, 1, j] = dplyr::filter(obs_df_filtered,
                                        date_step == j) %>%
      pull(states[i])
  }}
  
  return(obs_matrix)
}


```
#### Kalman filter function
```{r}
##' @param Y vector for holding states and parameters you're estimating
##' @param R observation error matrix
##' @param obs observations at current timestep
##' @param H observation identity matrix
##' @param n_en number of ensembles
##' @param cur_step current model timestep
kalman_filter = function(Y, R, obs, H, n_en, cur_step){
  
  cur_obs = obs[ , , cur_step]
  
  cur_obs = ifelse(is.na(cur_obs), 0, cur_obs) # setting NA's to zero so there is no 'error' when compared to estimated states
  
  ###### estimate the spread of your ensembles #####
  Y_mean = matrix(apply(Y[ , cur_step, ], MARGIN = 1, FUN = mean), nrow = length(Y[ , 1, 1])) # calculating the mean of each temp and parameter estimate
  delta_Y = Y[ , cur_step, ] - matrix(rep(Y_mean, n_en), nrow = length(Y[ , 1, 1])) # difference in ensemble state/parameter and mean of all ensemble states/parameters
  
  ###### estimate Kalman gain #########
  K = ((1 / (n_en - 1)) * delta_Y %*% t(delta_Y) %*% t(H[, , cur_step])) %*%
    qr.solve(((1 / (n_en - 1)) * H[, , cur_step] %*% delta_Y %*% t(delta_Y) %*% t(H[, , cur_step]) + R[, , cur_step]))
  
  #ugh getting error
  #delta_Y %*% t(delta_Y) %*% matrix(t(H[, , cur_step]))
  #non-conformable arguments
  #ok I think the issue is that Jake added the matrix function to the H matrix
  #transposition...not sure why this was done; probably ask TNM + RQT to take 
  #a look
  
  ###### update Y vector ######
  for(q in 1:n_en){
    Y[, cur_step, q] = Y[, cur_step, q] + K %*% (cur_obs - H[, , cur_step] %*% Y[, cur_step, q]) # adjusting each ensemble using kalman gain and observations
  }
  return(Y)
}

```
#### Function to initialize state and parameter vector
```{r}
#' initialize Y vector with draws from distribution of obs
#'
#' @param Y Y vector
#' @param obs observation matrix
initialize_Y = function(Y, obs, init_params, n_states_est, n_params_est, n_params_obs, n_step, n_en, state_sd, param_sd){
  
  # initializing states with earliest observations and parameters
  first_obs = coalesce(!!!lapply(seq_len(dim(obs)[3]), function(i){obs[,,i]})) #%>% # turning array into list, then using coalesce to find first obs in each position.
    #ifelse(is.na(.), mean(., na.rm = T), .) # setting initial temp state to mean of earliest temp obs from other sites if no obs
  #MEL omitting this for now - can build back in later if needed
  
  if(n_params_est > 0){
    ## update this later ***********************
    #MEL: not sure what update Jake was referring to here?
    first_params = init_params
  }else{
    first_params = NULL
  }
  
  #MEL: this is interesting; here we deal with the possibility of drawing negative
  #values from a normal distribution by just taking the absolute value of the
  #draw; I haven't seen this before; is this accepted practice in this type
  #of application?
  Y[ , 1, ] = array(abs(rnorm(n = n_en * (n_states_est + n_params_est),
                              mean = c(first_obs, first_params),
                              sd = c(state_sd, param_sd))),
                    dim = c(c(n_states_est + n_params_est), n_en))
  
  return(Y)
}
```
#### Function to create driver data matrix
```{r}
#' matrix for holding driver data
#'
#' @param drivers_df dataframe which holds all the driver data 
#' @param model_dates dates for model run 
#' @param n_drivers number of model drivers 
#' @param driver_colnames column names of the drivers in the driver dataframe 
#' @param driver_cv coefficient of variation for each driver data 
#' @param n_step number of model timesteps
#' @param n_en number of ensembles
get_drivers = function(drivers_df, model_dates, n_drivers, driver_colnames, driver_cv, n_step, n_en){
  
  drivers_filtered = drivers_df %>% 
    dplyr::filter(as.Date(datetime) %in% model_dates)

  drivers_out = array(NA, dim = c(n_step, n_drivers, n_en))
  
  for(i in 1:n_drivers){
    for(j in 1:n_step){
      drivers_out[j,i,] = rnorm(n = n_en, 
                               mean = as.numeric(drivers_filtered[j, driver_colnames[i]]),
                               sd = as.numeric(driver_cv[i] * drivers_filtered[j, driver_colnames[i]]))
    }
  }
  
  return(drivers_out) 
}

```
#### EnKF wrapper
```{r}
#' wrapper for running EnKF 
#' 
#' @param n_en number of model ensembles 
#' @param start start date of model run 
#' @param stop date of model run
#' @param time_step model time step, defaults to days 
#' @param obs_file observation file 
#' @param driver_file driver data file 
#' @param n_states_est number of states we're estimating 
#' @param n_params_est number of parameters we're estimating 
#' @param n_params_obs number of parameters for which we have observations 
#' @param decay_init initial decay rate of DOC 
#' @param obs_cv coefficient of variation of observations 
#' @param param_cv coefficient of variation of parameters 
#' @param driver_cv coefficient of variation of driver data for DOC Load, Discharge out, and Lake volume, respectively 
#' @param init_cond_cv initial condition CV 
#' @param state_names character string vector of state names as specified in obs_file

EnKF = function(n_en = 100, 
                start = '2018-06-05', # start date 
                stop = '2018-08-05', 
                time_step = 'days', 
                obs_file = lake_data,
                driver_file = lake_data,
                n_states_est = 2, 
                n_params_est = 1,
                n_params_obs = 0, 
                maxUptake_init = 1, 
                obs_cv = c(0.5,0.5),
                param_cv = 0.2,
                driver_cv = c(0.01, 0.01),
                init_cond_cv = c(1,1),
                state_names = c("chla","din")){
  
  
  n_en = n_en
  start = as.Date(start)
  stop = as.Date(stop)
  time_step = 'days' 
  dates = get_model_dates(model_start = start, model_stop = stop, time_step = time_step)
  n_step = length(dates)
  
  # get observation matrix
  obs_df = obs_file %>% 
    select(datetime, chla, din) 
  
  drivers_df = driver_file %>% 
    select(datetime, wtemp, par) 
  
  n_states_est = n_states_est # number of states we're estimating 
  
  n_params_est = n_params_est # number of parameters we're calibrating
  
  n_params_obs = n_params_obs # number of parameters for which we have observations
  
  maxUptake_init = maxUptake_init # Initial estimate of DOC decay rate day^-1 
  
  yini <- c( #initial estimate of PHYTO and DIN states, respectively
  PHYTO = 0.002, #mmolN m-3 which is the same as mg chl b/c ratio set to 1
  DIN = 9) #mmolN m-3
  
  state_cv = obs_cv #coefficient of variation of chla and din observations, respectively 
  state_sd = state_cv * yini
  init_cond_sd = init_cond_cv * yini
  
  param_cv = param_cv #coefficient of variation of maxUptake 
  param_sd = param_cv * maxUptake_init
  
  # driver data coefficient of variation for wtemp and PAR, respectively 
  driver_cv = driver_cv 
  
  
  # setting up matrices
  # observations as matrix
  obs = get_obs_matrix(obs_df = obs_df,
                       model_dates = dates,
                       n_step = n_step,
                       n_states = n_states_est,
                       states = state_names)
  
  # Y vector for storing state / param estimates and updates
  Y = get_Y_vector(n_states = n_states_est,
                   n_params_est = n_params_est,
                   n_step = n_step,
                   n_en = n_en)
  
  # observation error matrix
  R = get_obs_error_matrix(n_states = n_states_est,
                           n_params_obs = n_params_obs,
                           n_step = n_step,
                           state_sd = state_sd,
                           param_sd = param_sd)
  
  # observation identity matrix
  H = get_obs_id_matrix(n_states = n_states_est,
                        n_params_obs = n_params_obs,
                        n_params_est = n_params_est,
                        n_step = n_step,
                        obs = obs)
  
  # initialize Y vector
  Y = initialize_Y(Y = Y, obs = obs, init_params = maxUptake_init, n_states_est = n_states_est,
                   n_params_est = n_params_est, n_params_obs = n_params_obs,
                   n_step = n_step, n_en = n_en, state_sd = init_cond_sd, param_sd = param_sd)
  
  # interpolate missing driver data
  drivers_df = create_np_inputs(drivers_df = drivers_df,
                                driver_colnames = c("wtemp","par"))
  
  # get driver data with uncertainty - dim = c(n_step, driver, n_en) 
  drivers = get_drivers(drivers_df = drivers_df, 
                        model_dates = dates,
                        n_drivers = 2, 
                        driver_colnames = c('wtemp', 'par'), 
                        driver_cv = driver_cv, 
                        n_step = n_step, 
                        n_en = n_en) 
  
  # start modeling
  ## STOPPED EDITING HERE!
  ## next step is to incorporate NP model into this function instead of doc model
  for(t in 2:n_step){
    for(n in 1:n_en){
      
      # run model; 
      model_output = NP_model(TEMP = drivers[t-1, 1, n], 
                                      PHYTO = Y[1, t-1, n], 
                                      DIN = Y[2, t-1, n], 
                                      PAR = drivers[t-1, 2, n],
                                      maxUptake = Y[3, t-1, n])
      
      Y[1 , t, n] = model_output$PHYTO_pred # store in Y vector
      Y[2 , t, n] = model_output$DIN_pred
      Y[3 , t, n] = model_output$maxUptake
    }
    # check if there are any observations to assimilate 
    if(any(!is.na(obs[ , , t]))){
      Y = kalman_filter(Y = Y,
                        R = R,
                        obs = obs,
                        H = H,
                        n_en = n_en,
                        cur_step = t) # updating params / states if obs available
    }
  }
  out = list(Y = Y, dates = dates, drivers = drivers, R = R, obs = obs, state_sd = state_sd)
  
  return(out)
}
```

## Read in NEON data (using BARC as test case for now)
```{r}
#* Load in NEON sites dataframe ----
neon_sites_df <- read.csv("data/neon_sites.csv")
neon_sites_df$long <- round(neon_sites_df$long, 3)
neon_sites_df$lat <- round(neon_sites_df$lat, 3)

#* Reference for downloading variables ----
neon_vars <- read.csv("data/neon_variables.csv")

# Objective 1 - Site Selection ----
siteID <- "BARC"

#* Website to find more information:
print(paste0("https://www.neonscience.org/field-sites/", siteID))

# Objective 2 - Explore data ----
#* Load in data ----

#Drivers
wtemp <- read_neon_data(siteID, "Surface water temperature") %>%
  filter(value == 0.05 & year(Date) %in% c(2018,2019))
par <- read_neon_data(siteID, "Underwater PAR") %>%
  filter(year(Date) %in% c(2018,2019))

#States
chla <- read_neon_data(siteID, "Chlorophyll-a") %>%
  filter(year(Date) %in% c(2018,2019)) %>%
  mutate(value = value/1000) #to convert to mg for input into NP model
din <- read_neon_data(siteID, "Nitrogen") %>%
  filter(year(collectDate) %in% c(2018,2019)) %>%
  mutate(value = value*14*1000) #to convert to mmolN m-3 for input into NP model
#assuming this was analyzed as NO3/NO2-N and NH4-N and then summed, like we do
#should probably double-check that

#Plot data
p_wtemp <- ggplot(data = wtemp) +
  geom_point(aes(Date, V1)) +
  ylab("Temperature (\u00B0C)") +
  xlab("Time") +
  theme_classic()
ggplotly(p_wtemp)
#missing data from 05AUG18 to 19NOV18

p_par <- ggplot(data = par) +
  geom_point(aes(Date, value)) +
  ylab("PAR (umol/m2/s") +
  xlab("Time") +
  theme_classic()
ggplotly(p_par)

p_chla <- ggplot(data = chla) +
  geom_point(aes(Date, value)) +
  ylab("Chl-a mg/L") +
  xlab("Time") +
  theme_classic()
ggplotly(p_chla)
#looks like August 2020 or thereabouts might be a good dynamic time to pick
#and naturally the big bloom in 2018-2019 is when there is no wtemp data - ha!

p_din <- ggplot(data = din) +
  geom_point(aes(collectDate, value)) +
  ylab("DIN mmolN m-3") +
  xlab("Time") +
  theme_classic()
ggplotly(p_din)
#oohhhh but we don't have DIN then

#Check date ranges for various data
range(wtemp$Date, na.rm = TRUE)
range(par$Date, na.rm = TRUE)
range(chla$Date, na.rm = TRUE)
range(din$collectDate, na.rm = TRUE)
#ok so we are really looking at 2018-2019 probably

#ha what do we think the chances are that we'll find a time that overlaps in terms
#of data availability across all the NEON lakes??

#let's start with 05JUN18 to 05AUG18 just to see
```
## Data wrangling to get NEON data in format for Jake's functions
```{r}
wtemp1 <- wtemp %>%
  filter(Date >= "2018-06-04" & Date <= "2018-08-05") %>%
  rename(wtemp = V1) %>%
  select(Date,wtemp)

par1 <- par %>%
  filter(Date >= "2018-06-04" & Date <= "2018-08-05") %>%
  rename(par = value) %>%
  select(Date,par)

chla1 <- chla %>%
  filter(Date >= "2018-06-04" & Date <= "2018-08-05") %>%
  rename(chla = value) %>%
  select(Date,chla)

din1 <- din %>%
  filter(collectDate >= "2018-06-04" & collectDate <= "2018-08-05") %>%
  rename(din = value,
         Date = collectDate) %>%
  select(Date,din) %>%
  group_by(Date) %>%
  summarize(din = mean(din, na.rm = TRUE))
#there are multiple obs on one day - why??
#for now, just taking average of all obs on same day

lake_data_00 <- left_join(par1,wtemp1,by = "Date")
lake_data_0 <- left_join(lake_data_00,chla1,by = "Date")
lake_data <- left_join(lake_data_0,din1,by = "Date") %>%
  rename(datetime = Date)

```

## Run forecast
```{r}
n_en = 100 # how many ensembles 


##MAKE SURE TO REVISIT ALL THE CVS CAUSE YOU WERE THINKING IN TERMS OF SD WHEN
#YOU ESTIMATED THESE ORIGINALLY
est_out = EnKF(n_en = n_en, 
           start = '2018-06-05', # start date 
           stop = '2018-08-05', # stop date
           time_step = 'days',
           obs_file = lake_data,
           driver_file = lake_data,
           n_states_est = 2, 
           n_params_est = 1,
           n_params_obs = 0,
           maxUptake_init = 1.0, 
           obs_cv = c(0.2,0.2),#cv for chl-a and DIN, respectively
           param_cv = 0.2,#for maxUptake
           driver_cv = c(0.2,0.2), # CV of driver data for wtemp and PAR, respectively 
           init_cond_cv = c(0.2,0.2),#cv for chl-a and DIN, respectively
           state_names = c("chla","din"))


```
##Plot forecast
```{r}
# plotting 
plot_chla = function(est_out){
  mean_chla_est = apply(est_out$Y[1,,] , 1, FUN = mean)
  plot(mean_chla_est ~ est_out$dates, type ='l', 
       ylim = range(c(est_out$Y[1,,] , 
                      est_out$obs[1,,] ),
                    na.rm = T),
       col = 'grey', ylab = 'chla (mg L-1)', xlab = '')
  for(i in 2:n_en){
    lines(est_out$Y[1,,i]  ~ est_out$dates, 
          col = 'grey')
  }
  lines(mean_chla_est ~ est_out$dates, col = 'black', lwd =2 )
  points(est_out$obs[1,,]  ~ 
           est_out$dates, pch = 16, col = 'red')
  arrows(est_out$dates, est_out$obs[1,,]  - est_out$state_sd[1], 
         est_out$dates, est_out$obs[1,,]  + est_out$state_sd[1], 
         code = 3, length = 0.1, angle = 90, col = 'red')
}
plot_chla(est_out) 

# plotting 
plot_din = function(est_out){
  mean_din_est = apply(est_out$Y[2,,] , 1, FUN = mean)
  plot(mean_din_est ~ est_out$dates, type ='l', 
       ylim = range(c(est_out$Y[2,,] , 
                      est_out$obs[2,,] ),
                    na.rm = T),
       col = 'grey', ylab = 'chla (mg L-1)', xlab = '')
  for(i in 2:n_en){
    lines(est_out$Y[2,,i]  ~ est_out$dates, 
          col = 'grey')
  }
  lines(mean_din_est ~ est_out$dates, col = 'black', lwd =2 )
  points(est_out$obs[2,,]  ~ 
           est_out$dates, pch = 16, col = 'red')
  arrows(est_out$dates, est_out$obs[2,,]  - est_out$state_sd[2], 
         est_out$dates, est_out$obs[2,,]  + est_out$state_sd[2], 
         code = 3, length = 0.1, angle = 90, col = 'red')
}
plot_din(est_out) 
```
